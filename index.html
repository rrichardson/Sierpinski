<html>
<body>

<h1>Fractals for Fun and Profit</h1>

<p>
Unlike me, my wife uses computers for serious calculations to answer important questions.
One of these problems involves taking the cross-correlations of the continuous seismic data of hundreds of measuring stations.
A computation that can take days or even weeks.
Interestingly, it turns out that a large part of this time is spent loading data from disks into memory.
To better understand the problem, let us take a high-level look at the calculation of cross-correlations in Pythonesque pseudo-code:
</p>

<pre>
# given is some list of stations, say a list of names of the data files
stations = [..]

# correlate, one by one, each unique pair of stations
for i in range(len(stations)) :
    for j in range(i+1, range(len(stations))) :
        # the following correlates both the pair (i,j) and (j,i)
        computation(stations[i], stations[j])
</pre>

<p>
The pseudo-code traverses all possible combinations of two stations and computes the so called cross-correlation.
Each station's signal consists of a large data-file that is stored on disk and must be loaded into memory to compute the correlation.
Loading memory from disk is an expensive operation, because even modern disks are slow compared to the processor and memory.
Luckily the system caches data, i.e., it reuses data that is still stored in memory in stead of reloading it.
The memory has limited capacity.
When the memory is full, data that is not yet in the memory will replace the least recently used data.
</p>

<p>
Thus the order in which we access the station's data - the so called schedule - determines the number of disk accesses.
The question is, what schedule minimizes the number of disk reads and maximize the use of the cache.
</p>


<h2>Computing the Cost of a Schedule</h2>

<p>
The schedule can be visualized as a path through a square matrix where each cell consists of a combination of two stations.
Such a combination is called a job.
Only the lower (or upper) triangle of the matrix need to be visited as we consider (3,4) a duplicate of (4,3).
The pseudo-code above consists in the following schedule that we call the scanline schedule.
</p>

<center><iframe src="ScanlineSchedule.html" width=400 height=400 frameborder=0></iframe></center>

<p>
The displayed schedule thus starts correlating station 1 with itself, then correlates 1 with 2, 2 with 2, etc.
The cost - in terms of the number of disk accesses or cache misses - of this schedule can be computed by simulating the execution while keeping track of the cache's contents and counting the number of times we load a station's data that is not yet in memory.
Running such a simulation with memory capacity for five elements gives the following cumulative cost for the successive executions.
</p>

<center><iframe src="ScanlineCost.html" width=600 height=400 frameborder=0></iframe></center>

<p>
The cumulative cost shows that the scanline schedule initially (up to 15) performs quite well, but once the horizontal stretches of the schedule are longer than 5 each new job results in a cache miss.
In fact, one can do quite a bit better by making shorter stretches.
Give it a try!
</p>

<center><iframe src="ScheduleGame.html" width=600 height=400 frameborder=0></iframe></center>

<p>
The above little applet allows you to create a schedule by clicking on the jobs and computes the total number of disk accesses at the time of each successive job in the schedule.
Also shown is are the contents of the cache after execution of the last job in the schedule.
To compute the number of disk accesses we assume the memory can cache the data of 5 stations and always replaces the least recently used element in cache.
</p>


<h2>The Sierpinsky Curve or H-Index</h2>

<p>
It turns out to be quite challenging to find the best schedule with the least disk accesses.
Some experimenting also shows that the performance of a schedule changes quite a bit for different memory sizes.
That complicates our task, especially considering that the available memory often changes during execution. 
For example, when the correlations are done on a supercomputer that is shared by other scientists running similar experiments.
</p>

<p>
However, one quickly notices that it is generally better not to make large jumps.
That is, to make a schedule where the most recent jobs are close together in the matrix.
This leads to a divide and conquer strategy, where we subdivide the triangle of jobs into smaller triangles and schedule the jobs in the first triangle before the second.
Applying this pattern recursively, we get the following sequence.
</p>


<center><iframe src="SierpinskyCurve.html" width=300 height=300 frameborder=0></iframe></center>

<p>
The schedule based on the Sierpinsky curve doesn't look quite the same because we cannot properly subdivide the triangle that consists of discrete jobs.
In stead, we use a variant that is called the H-index.
In this context "index" is another word for schedule and the H stands for the shape the pattern takes: there's an H on its side in the lower right corner.
</p>

<center><iframe src="HIndex.html" width=400 height=400 frameborder=0></iframe></center>

<p>
How much better does the H-index do?
The following graph plots the cost in termes of cache misses of each schedule for a case with 64 stations and therefore 576 jobs for varying amounts of memory.
</p>

<center><iframe src="PerformancePlot.html" width=600 height=400 frameborder=0></iframe></center>

<p>
Note that if memory has capacity for 64 stations all schedules will have the same cost because every job has to be loaded exactly once.
<p>


<h2>Conclusion</h2>

optimality

boundary conditions (not powers of two)


<h2>Post Scriptum</h2>

other indexing schemes (locality)

particularity of stations: both dimensions of the matrix are the same stations: doesn't matter asymptotically as log as memory is considerably smaller than the number of stations

<p>
This leads us to the realm of <a href="http://en.wikipedia.org/wiki/Cache-oblivious_algorithm">cache oblivious algorithms</a> that perform well, without any knowledge of the memory's capacity.
</p>

other caching strategies
</body>
</html>

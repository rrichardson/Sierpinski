<html>
<body>

<h1>Fractals for Fun and Profit</h1>

<p>
Unlike me, my wife uses computers for serious calculations to answer important questions.
One of these problems involves taking the cross-correlations of the continuous seismic data of hundreds of measuring stations.
A computation that can take days or even weeks.
Interestingly, it turns out that a large part of this time is spent loading data from disks into memory.
To better understand the problem, let us take a high-level look at the calculation of cross-correlations in Pythonesque pseudo-code:
</p>

<pre>
# given is some list of stations, say a list of names of the data files
stations = [..]

# correlate, one by one, each unique pair of stations
for i in range(len(stations)) :
    for j in range(i+1, range(len(stations))) :
        # the following correlates both the pair (i,j) and (j,i)
        computation(stations[i], stations[j])
</pre>

<p>
The pseudo-code traverses all possible combinations of two stations and computes the so called cross-correlation.
Each station's signal consists of a large data-file that is stored on disk and must be loaded into memory to compute the correlation.
Loading memory from disk is an expensive operation, because even modern disks are slow compared to the processor and memory.
Luckily the system caches data, i.e., it reuses data that is still stored in memory in stead of reloading it.
The memory has limited capacity.
When the memory is full, data that is not yet in the memory will replace the least recently used data.
</p>

<p>
Thus the order in which we access the station's data - the so called schedule - determines the number of disk accesses.
The question is, what schedule minimizes the number of disk reads and maximize the use of the cache.
</p>


<h2>Computing the Cost of a Schedule</h2>

<p>
The schedule can be visualized as a path through a square matrix where each cell consists of a combination of two stations.
Such a combination is called a job.
Only the lower (or upper) triangle of the matrix need to be visited as we consider (3,4) a duplicate of (4,3).
The pseudo-code above consists in the following schedule that we call the scanline schedule.
</p>

<center><a name="ScanlineSchedule"><iframe src="ScanlineSchedule.html" width=400 height=400 frameborder=0></iframe></a></center>

<p>
The displayed schedule thus starts correlating station 1 with itself, then correlates 1 with 2, 2 with 2, etc.
The cost - in terms of the number of disk accesses or cache misses - of this schedule can be computed by simulating the execution while keeping track of the cache's contents and counting the number of times we load a station's data that is not yet in memory.
Running such a simulation with memory capacity for five elements gives the following cumulative cost for the successive executions.
</p>

<center><a name="ScanlineCost"><iframe src="ScanlineCost.html" width=600 height=400 frameborder=0></iframe></a></center>

<p>
The cumulative cost shows that the scanline schedule initially (up to 15) performs quite well, but once the horizontal stretches of the schedule are longer than 5 each new job results in a cache miss.
In fact, one can do quite a bit better by making shorter stretches.
Give it a try!
</p>

<center><a name="ScheduleGame"><iframe src="ScheduleGame.html" width=600 height=400 frameborder=0></iframe></a></center>

<p>
The above little applet allows you to create a schedule by clicking on the jobs and computes the total number of disk accesses at the time of each successive job in the schedule.
Also shown is are the contents of the cache after execution of the last job in the schedule.
To compute the number of disk accesses we assume the memory can cache the data of 5 stations and always replaces the least recently used element in cache.
</p>


<h2>The Sierpinsky Curve or H-Index</h2>

<p>
It turns out to be quite challenging to find the best schedule with the least disk accesses.
Some experimenting also shows that the performance of a schedule changes quite a bit for different memory sizes.
That complicates our task, especially considering that the available memory often changes during execution. 
For example, when the correlations are done on a supercomputer that is shared by other scientists running similar experiments.
</p>

<p>
However, one quickly notices that it is generally better not to make large jumps.
That is, to make a schedule where the most recent jobs are close together in the matrix.
This leads to a divide and conquer strategy, where we subdivide the triangle of jobs into smaller triangles and schedule the jobs in the first triangle before the second.
Applying this pattern recursively, we get the following sequence.
</p>


<center><a name="SierpinskyCurve"><iframe src="SierpinskyCurve.html" width=300 height=300 frameborder=0></iframe></a></center>

<p>
The schedule based on the Sierpinsky curve doesn't look quite the same because we cannot properly subdivide the triangle that consists of discrete jobs.
In stead, we use a variant that is called the H-index.
In this context "index" is another word for schedule and the H stands for the shape the pattern takes: there's an H on its side in the lower right corner.
</p>

<center><a name="HIndex"><iframe src="HIndex.html" width=400 height=400 frameborder=0></iframe></a></center>

<p>
How much better does the H-index do?
The following graph plots the cost in termes of cache misses of each schedule for a case with 64 stations and therefore 576 jobs for varying amounts of memory.
</p>

<center><a name="PerformancePlot"><iframe src="PerformancePlot.html" width=600 height=400 frameborder=0></iframe></a></center>

<p>
The range is limited from 2 to 64: one needs to be able to store at least two and if memory has capacity for 64 stations all schedules will have cost 64 because every job has to be loaded exactly once.
The cost of the scanline algorithm is dominated by a linear relation also seen in the last part (after job 16) in the <a href="#ScanlineCost">previous job</a>.
One may also note that the H-index performs best when the memory capacity is a power of two, note in particular the low points at 4, 8 and 16.
In between those spots the H-index becomes slightly irregular but generally the plotted curve shows a logarithmic 
<p>


<h2>Conclusion</h2>

Hopefully the <a href="#PerformancePlot">final plot</a> convinces you that the Sierpinski curve, or more accurately, H-index performs much better than the scanline schedule.
However, I have not proven that the H-index is the optimal schedule.
In fact, one can easily show that for certain memory sizes, such as in the <a href="#ScheduleGame">manual scheduling exercise</a>, it is possible to do (slightly) better than the H-index.
<a href="http://www.sciencedirect.com/science/article/pii/S0166218X00003267">Rolf Niedermeiera, Klaus Reinhardt and Peter Sanders</a>, who invented the H-index, have given strong evidence that it is asymptotically optimal.


<h2>Post Scriptum</h2>

An attentive reader may have noticed that I've only used examples where the number of stations is a power of two.
This is no coincidence: the H-index is really only defined for those sizes.
One can, however, adapt the index for larger sizes in ways that most of the schedule will be according to the h-index.
Exploring the different solutions would be an interesting project.

</body>
</html>

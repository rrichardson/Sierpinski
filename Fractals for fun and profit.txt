#
Fractals for fun and profit

Abstract

#Even among computer scientists, fractals are known best for the pretty pictures mad famous by the late Mandelbrot.
#But fractals can be as useful as they are pretty.
This text explains the use of the Sierpinsky curve --a fractal pattern-- to speed up the computation of self-joins, the cross product of a collection with itself, on multi-core systems.
In particular, scheduling according to the Sierpinsky curve is shown to require less working memory and exhibit better caching behavior than a naive implementation.


Introduction

The self-join is a often recurring problem in computing intensive programs.
The term ``join'' derives from databases, but I noticed the problem elsewhere.
Unlike me, my wife uses computers for serious calculations to solve big problems.
One of these problems involves taking the cross-correlations of the continuous seismic data of hundreds of stations.
Through these cross correlations, seismologists can observe changes in the Earth's crust using background noise created by the oceans.
This relatively new technique has the advantage that the noise never stops, while other observations depend on ``events,'' seismologists' euphemism for earthquakes.
Essentially they go from pictures to moving images, a potential revolution in our understanding of the Earth's crust.
#TODO: 'essentially, they are able to continuously monitor what is going on beneath the Earth surface, a potential revolution..'
Potential, because a lot of work is still needed to improve the quality of the observations.

One relatively simple way to improve the quality, would be to use more stations.
Unfortunately, the cost of calculations is exponential in the number of stations.
The biggest cost factor is the sheer amount of data that has to be processed.

To better understand the problem, let us take a high-level look at the calculation of cross-correlations in pseudo-code.

# given is some list of stations, say a list of names of the data files
stations = [..]

# correlate, one by one, each unique pair of stations
for i in range(len(stations)) :
    for j in range(i, range(len(stations))) :
        # the following correlates both the pair (i,j) and (j,i)
        correlate(stations[i], stations[j])

Although improvements can be made even here, we will look at a more interesting case where multiple cores are running in parallel.
The idea is that we prepare a list of station pairs to be correlated.
Each processor core takes an element from the list, calculates the correlation and takes the next available element from the list until the list is empty.

In this text we prepare the list of station pairs, henceforth job list, in advance or statically as it is called.
But there is no reason why the items of the list couldn't be calculated dynamically when needed.

The simplest way to prepare a job list, would be to imitate the above sequential crosscorrelation, leading to the following program.

for i in range(len(stations)):
    for j in range(i, len(stations)) :
        schedule(stations[i],stations[j])

The main problem with this implementation, is the number of station data files that is loaded during execution.
A fact best illustrated by an example.

...


Analysis of the Working Set


